{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a140edef-a2ec-4b54-9982-43d50f920ff9",
   "metadata": {},
   "source": [
    "# Super Resolution Auto Encoder\n",
    "Super Resoution model based on tng simulated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a5bc6-611c-4545-9f7a-61f3af62d78c",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ee207-277f-45cf-ac62-86a3e6ecfb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 13 17:49:10 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    24W / 250W |  10898MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8    22W / 250W |  10898MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:AF:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    23W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8    16W / 250W |  10898MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    412586      C   ...al/jupyterhub/bin/python3    10895MiB |\n",
      "|    1   N/A  N/A    153205      C   ...al/jupyterhub/bin/python3    10895MiB |\n",
      "|    3   N/A  N/A    413158      C   ...al/jupyterhub/bin/python3    10895MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if gpu is free\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e3fc58-aac8-4b38-a44e-444b1cc811b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamsweere\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch uses device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "data_dir = \"/home/ssweere/data/sim\" #faster local storage, do not use too much\n",
    "dataset_dir = \"tng300_2048\"\n",
    "root_dir = os.path.join(data_dir, dataset_dir, \"fits\")\n",
    "   \n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: uncomment if you want to train on gpu, check if the gpu is free before you enable it\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"PyTorch uses device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e419c-aa71-48ad-989d-c82cf884ba6a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67933b4-061c-4c8f-8b63-d7408060fa8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import (imshow_norm, MinMaxInterval,\n",
    "                                   SqrtStretch)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "def plot_img(img, title=None, figsize =(10,10)):\n",
    "    # Generate and display a test image\n",
    "    # image = np.arange(65536).reshape((256, 256))\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.autoscale(enable=True)\n",
    "\n",
    "    pos = ax.imshow(img, cmap='hot', interpolation=None)\n",
    "    fig.colorbar(pos, ax=ax)\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def plot_multiple_img(images, titles = None, rows=None, cols=None, figsize=(10,10), merge=False, colorbar = True, save_fig=None, show_plot=True, wandb_log=False):\n",
    "    \"\"\"Plot multiple images\n",
    "\n",
    "    Args:\n",
    "        images (list): List of images as numpy arrays.\n",
    "        titles (list) (optional): The titles of the subplots.\n",
    "        rows (int) (optional): The amount of rows in the subplot, if this is set and cols is None it will expand in the cols direction\n",
    "        cols (int) (optional): The amount of cols in the subplot, if this is set and rows is None it will expand in the rows direction\n",
    "        figsize (tuple) (optional): The figsize of the subplots\n",
    "        merge (boolean) (optional): If this parameter is set all the images are compressed into one big image and displayed\n",
    "        colorbar (boolean) (optional): If this parameter is set the colorbar is shown\n",
    "        save_fig (string) (optional): If this parameter is set save the image to a file with the given name\n",
    "        show_plot (boolean) (optional): If this parameter is set the plot is displayed\n",
    "        wandb_log (boolean) (optional): If this parameter is set the plot is logged to wandb\n",
    "    \"\"\"\n",
    "        \n",
    "    # Determine the rows and cols if they are set to none\n",
    "    if not rows and not cols:\n",
    "        # Go for a square layout if possible, if not possible expand in the x direction\n",
    "        s_dim = np.sqrt(len(images))\n",
    "        if s_dim.is_integer():\n",
    "            # The number of images is perfect for a square layout\n",
    "            rows = int(s_dim)\n",
    "            cols = int(s_dim)\n",
    "        else:\n",
    "            # The number of images is not perfect for a square layout\n",
    "            # Expand in the x direction\n",
    "            rows = int(np.floor(s_dim))\n",
    "            cols = int(np.ceil(len(images)/float(rows)))\n",
    "    elif not rows and cols:\n",
    "        # rows is None and cols is not, expand in the rows direction\n",
    "        rows = int(np.ceil(len(images)/float(cols)))\n",
    "    elif rows and not cols:\n",
    "        # cols is None and rows is not, expand in the cols direction\n",
    "        cols = int(np.ceil(len(images)/float(rows)))\n",
    "    else:\n",
    "        if len(images) > rows*cols:\n",
    "            raise ValueError(f\"The specified rows ({rows}) and cols ({cols}) do not provide enough room for all the images ({rows*cols}/{len(images)})\")\n",
    "            \n",
    "    if merge:\n",
    "        counter = 0\n",
    "        img_merged = None\n",
    "        for y in range(rows):\n",
    "            # First merge the images in x direction\n",
    "            img_m_x = None\n",
    "            for x in range(cols):\n",
    "                img = images[counter]\n",
    "                counter += 1\n",
    "                \n",
    "                if img_m_x is None:\n",
    "                    img_m_x = img\n",
    "                else:\n",
    "                    img_m_x = np.hstack((img_m_x, img))\n",
    "            \n",
    "            # Then stack them vertically on the big image\n",
    "            if img_merged is None:\n",
    "                img_merged = img_m_x\n",
    "            else:\n",
    "                img_merged = np.vstack((img_merged, img_m_x))\n",
    "         \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.autoscale(enable=True)\n",
    "        if titles is not None:\n",
    "            ax.set_title(' - '.join(titles))\n",
    "\n",
    "        pos = ax.imshow(img_merged, cmap='hot', interpolation=None)\n",
    "        if colorbar:\n",
    "            fig.colorbar(pos, ax=ax)\n",
    "                \n",
    "                \n",
    "    else: \n",
    "        fig, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "        axs = np.array(axs)\n",
    "        if len(axs.shape) == 1:\n",
    "            # We only have one row, expand the dimensions in order to have the same structure \n",
    "            # as with multiple rows\n",
    "            axs = np.expand_dims(axs, axis = 1)\n",
    "\n",
    "        counter = 0\n",
    "        for x in range(axs.shape[0]):\n",
    "            for y in range(axs.shape[1]):\n",
    "    #             if rows == 1:\n",
    "    #                 # This is a fix for when we only have one row\n",
    "    #                 ax = axs[x][y]\n",
    "    #             else:\n",
    "    #                 ax = axs[y][x]\n",
    "                ax = axs[x, y]\n",
    "\n",
    "                idx = counter\n",
    "                counter += 1\n",
    "\n",
    "                # Check if there are enough images to show, if not continue\n",
    "                if idx >= len(images):\n",
    "                    continue\n",
    "\n",
    "                img = images[idx]\n",
    "                if titles is not None:\n",
    "                    ax.set_title(titles[idx])\n",
    "\n",
    "                pos = ax.imshow(img, cmap='hot', interpolation=None)\n",
    "                \n",
    "                if colorbar:\n",
    "                    # create an axes on the right side of ax. The width of cax will be 5%\n",
    "                    # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "                    divider = make_axes_locatable(ax)\n",
    "                    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "                    plt.colorbar(pos, cax=cax)\n",
    "\n",
    "        # Correct for a nicer layout\n",
    "        fig.tight_layout()\n",
    "#     fig.subplots_adjust(hspace=-0.7)\n",
    "    \n",
    "    if save_fig is not None:\n",
    "        plt.savefig(save_fig, bbox_inches='tight')\n",
    "#         print(\"Saved figure to\",save_fig)\n",
    "    \n",
    "    if wandb_log:\n",
    "        wandb.log({\"images\": plt})\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc816a-fad6-4271-a52d-96cdccd419a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Creation\n",
    "Based on: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e832f48d-0826-43f7-b94c-df922fea5604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsampleSum(object):\n",
    "    \"\"\"Downsample the image summing the values using conv2d\n",
    "    \n",
    "    Args:\n",
    "        output_size (int): Desired output size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Create an input_size and weigths parameter to cache previous weights\n",
    "        self.input_size = 0\n",
    "        self.kernel_size = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        input_size = img.shape[-1]\n",
    "        \n",
    "        if input_size == self.output_size:\n",
    "            # The sizes are the same, return itself\n",
    "            return img\n",
    "        \n",
    "        if input_size != self.input_size or self.input_size == 0 or self.kernel_size is None or self.weights is None:          \n",
    "            # New input_size we cannot use the cache\n",
    "            self.input_size = input_size\n",
    "            \n",
    "            if input_size == self.output_size:\n",
    "                raise ValueError(f\"The desired output size {output_size} is the same as the input size {input_size}.\")\n",
    "\n",
    "            kernel_size = input_size/self.output_size\n",
    "\n",
    "            if kernel_size%2 != 0:\n",
    "                raise ValueError(f\"The desired output size {output_size} is not a multiple of 2 of the input size {input_size}\")\n",
    "\n",
    "            # The kernel size seems to be valid, convert it to int\n",
    "            self.kernel_size = int(kernel_size)\n",
    "            self.input_size = input_size\n",
    "\n",
    "            # Generate the weights for the conv2d\n",
    "            weights = torch.ones((self.kernel_size, self.kernel_size))\n",
    "            self.weights = weights.view(1, 1, self.kernel_size, self.kernel_size).repeat(1, 1, 1, 1)\n",
    "\n",
    "        # The conv2d needs the data to be in minibatches and have dimensions [1, x, x]\n",
    "        x = torch.unsqueeze(img, axis=0)\n",
    "        x = torch.unsqueeze(x, axis=0)\n",
    "        \n",
    "        output = torch.nn.functional.conv2d(x, self.weights, stride=self.kernel_size)\n",
    "        \n",
    "        # Return the result from the minibatch\n",
    "        return output[0][0]\n",
    "\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        crop_p (float, list): Desired crop percentage, if list it will randomly sample the crop percentage from the list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, crop_p):\n",
    "        assert isinstance(crop_p, (float, list))\n",
    "        self.crop_p = crop_p\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        if type(self.crop_p) is list:\n",
    "            crop_p = np.random.choice(self.crop_p)\n",
    "        else:\n",
    "            crop_p = self.crop_p\n",
    "        \n",
    "        new_h = int(h*crop_p)\n",
    "        new_w = int(w*crop_p)\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)       \n",
    "        \n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize and image optionally based on a strectching function. First apply the strectching function then normalize to max = 1. \n",
    "        The minimum possible value is 0 independent of the image, the image is thus only normalized based on the max value.\n",
    "        Returns the normalized image and the max value\n",
    "\n",
    "    Args:\n",
    "         stretch_f (string) (optional) : The stretching function options: linear, sqrt. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stretch_f=\"linear\"):\n",
    "        assert isinstance(stretch_f, str)\n",
    "        if stretch_f == 'linear':\n",
    "            self.stretch_f = torch.nn.Identity\n",
    "        elif stretch_f == 'sqrt':\n",
    "            self.stretch_f = torch.sqrt\n",
    "        else:\n",
    "            raise ValueError(f\"Stretching function {stretch_f} is not implemented\")\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Calculate the max value\n",
    "        max_val = torch.max(image)\n",
    "        \n",
    "        # Normalize the image\n",
    "        image = image/max_val\n",
    "        \n",
    "        # Apply the stretching function\n",
    "        image = self.stretch_f(image)\n",
    "\n",
    "        return image, max_val.item()\n",
    "\n",
    "class DeNormalize(object):\n",
    "    \"\"\"DeNormalize and image optionally based on the originala strectching function and the original max value. \n",
    "        Returns the denormalized image\n",
    "\n",
    "    Args:\n",
    "         stretch_f (string) (optional) : The stretching function options: linear, sqrt. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stretch_f=\"linear\"):\n",
    "        assert isinstance(stretch_f, str)\n",
    "        if stretch_f == 'linear':\n",
    "            self.stretch_f = torch.nn.Identity\n",
    "        elif stretch_f == 'sqrt':\n",
    "            self.stretch_f = lambda x: torch.pow(x, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Stretching function {stretch_f} is not implemented\")\n",
    "\n",
    "    def __call__(self, image, max_val):\n",
    "        # Aply the opposite stretching function\n",
    "        image = self.stretch_f(image)\n",
    "       \n",
    "        # multiply the image with the max val\n",
    "        image = image*max_val\n",
    "        \n",
    "        return image\n",
    "    \n",
    "class TngDataset(Dataset):\n",
    "    \"\"\"Illustrius TNG X ray simulated images dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, lr_res, hr_res, normalize=None, transform=None, preload=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the fits images.\n",
    "            lr_res (int): The low resolution\n",
    "            hr_res (int): The high resolution\n",
    "            normalize (string) (optional): The normalization method, options: linear, sqrt. \n",
    "                                            When None no normalization will be done\n",
    "            transform (callable) (optional): Optional transform to be applied\n",
    "                                                on a sample.\n",
    "            preload (boolean) (optinal): Preload the data into ram\n",
    "                \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.lr_res = lr_res\n",
    "        self.hr_res = hr_res\n",
    "        self.transform = transform\n",
    "        self.stretch_f = normalize\n",
    "        self.preload = preload\n",
    "        \n",
    "        self.fits_files = []\n",
    "        # Only save the files that end with .fits\n",
    "        for file in os.listdir(root_dir):\n",
    "            if file.endswith(\".fits\"):\n",
    "                self.fits_files.append(file)\n",
    "                \n",
    "        if preload:\n",
    "            # Preload all the image\n",
    "            print(\"Preloading the fits images\")\n",
    "            self.fits_images = []\n",
    "            \n",
    "            for i in tqdm(range(len(self.fits_files))):\n",
    "                fits_file = self.fits_files[i]\n",
    "                self.fits_images.append(self.load_fits(fits_file))\n",
    "        \n",
    "        # Create the downsample sum classes\n",
    "        self.downsample_lr = DownsampleSum(output_size=lr_res)\n",
    "        self.downsample_hr = DownsampleSum(output_size=hr_res)\n",
    "        \n",
    "        if normalize:\n",
    "            # Create the normalization class\n",
    "            self.normalize = Normalize(normalize)\n",
    "        else:\n",
    "            self.normalize = None\n",
    "        \n",
    "    def load_fits(self, fits_file):\n",
    "        hdu = fits.open(os.path.join(self.root_dir, fits_file))\n",
    "        # Extract the image data from the fits file and convert to float \n",
    "        # (these images will be in int but since we will work with floats in pytorch we convert them to float)\n",
    "        img = hdu['PRIMARY'].data.astype(np.float32)\n",
    "        hdu.close()\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fits_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        if self.preload:\n",
    "            fits_file = self.fits_files[idx]\n",
    "            # Load the preloaded image\n",
    "            img = self.fits_images[idx]\n",
    "        else:\n",
    "            fits_file = self.fits_files[idx]\n",
    "            img = self.load_fits(fits_file)\n",
    "        \n",
    "        # Convert the img to tensor\n",
    "        img = torch.tensor(img)\n",
    "        \n",
    "        # Apply the transformations\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Downsample to the desired image resolutions\n",
    "        lr_img = self.downsample_lr(img)\n",
    "        hr_img = self.downsample_hr(img)\n",
    "        \n",
    "        # Apply the normalization\n",
    "        if self.normalize:\n",
    "            lr_img, lr_max = self.normalize(lr_img)\n",
    "            hr_img, hr_max = self.normalize(hr_img)\n",
    "        else:\n",
    "            lr_max = torch.max(lr_img).item()\n",
    "            hr_max = torch.max(hr_img).item()\n",
    "        \n",
    "        # Torch needs the data to have dimensions [1, x, x]\n",
    "        lr_img = torch.unsqueeze(lr_img, axis=0)\n",
    "        hr_img = torch.unsqueeze(hr_img, axis=0)\n",
    "        \n",
    "        sample = {'lr': lr_img, 'hr': hr_img, 'source': fits_file, 'lr_max': lr_max, 'hr_max': hr_max, 'stretch_f': self.stretch_f}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61be4c29-f8dd-42f0-ac71-d78ebe9aa5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sample(sample):\n",
    "    # Plot one sample\n",
    "    images = [sample['lr'].numpy(), sample['hr'].numpy()]\n",
    "    if len(images[0].shape) == 3:\n",
    "        # The dimensions are probably [1, x, x]. Make them [x, x]\n",
    "        images = [x[0] for x in images]\n",
    "        \n",
    "    titles = [f\"lr {images[0].shape}\",f\"hr {images[1].shape}\"]\n",
    "    plot_multiple_img(images = images, titles = titles)\n",
    "    \n",
    "def preview_dataset(dataset, n=10):\n",
    "    # Plot n samples randomly chosen from the dataset\n",
    "    for i in range(n):\n",
    "        index = np.random.randint(len(dataset))\n",
    "        sample = dataset[index]\n",
    "        plot_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2c3c3-3655-4c0a-966d-6556e8504df3",
   "metadata": {},
   "source": [
    "## Create the dataset and load the data\n",
    "For all the possible prebuild transforms see: https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc229d-4cfc-4bf4-b664-6e39a8606154",
   "metadata": {},
   "source": [
    "# TODO: the current image strechting and normalization is probably not suitable for the reconversion to fits, this is a temporary solution. This is also per image normalization, not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb37e1c-08b4-4d23-b4cd-cddeb2d75023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset settings\n",
    "# lr_res = 32\n",
    "# hr_res = 128\n",
    "# batch_size = 64\n",
    "# crop_p = [0.25, 0.5]\n",
    "# # crop_p = [0.125, 0.25, 0.5] #This will random sample from multiple crop percentages\n",
    "# normalize = 'sqrt' #Other options None and 'linear'\n",
    "# # normalize = None\n",
    "# preload = False #Warning this increases the ram usage dramatically\n",
    "\n",
    "# # Prepare the data transforms\n",
    "# data_transform = transform=transforms.Compose([\n",
    "#                                         RandomCrop(crop_p = crop_p),\n",
    "#                                         transforms.RandomHorizontalFlip(),\n",
    "#                                         transforms.RandomVerticalFlip(),\n",
    "#                                     ])\n",
    "\n",
    "# tng_dataset = TngDataset(root_dir = root_dir, lr_res = lr_res, hr_res = hr_res, normalize=normalize, transform=data_transform, preload=preload)\n",
    "\n",
    "# train_val_test_split = [0.7, 0.15, 0.15]\n",
    "# train_len = int(len(tng_dataset)*train_val_test_split[0])\n",
    "# val_len = int(len(tng_dataset)*train_val_test_split[1])\n",
    "# test_len = len(tng_dataset) - train_len - val_len\n",
    "\n",
    "# # Note that the test set has the same transformations as the train an validation set\n",
    "# tng_datasets = torch.utils.data.random_split(tng_dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "# tng_datasets = {'train':tng_datasets[0], 'val':tng_datasets[1], 'test':tng_datasets[2]}\n",
    "\n",
    "# print(\"Train set size:\", len(tng_datasets['train']))\n",
    "# print(\"Validation set size:\", len(tng_datasets['val']))\n",
    "# print(\"Test set size:\", len(tng_datasets['test']))\n",
    "\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(tng_datasets[x], batch_size=batch_size,\n",
    "#                                              shuffle=True, num_workers=0)\n",
    "#               for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c4119-9ae5-4f52-9002-7a35298e91a1",
   "metadata": {},
   "source": [
    "## Visualize a few images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ec26a5-d7a1-4fef-98ed-1ad4a1f8ec23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_sample(tng_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4674d951-6091-412f-8d56-89e2ad11dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tng_datasets['train'][0]['lr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9768653b-58f2-47cc-9d1d-e5333b2ba09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview_dataset(tng_datasets['train'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b38df3-f180-4c7d-a100-8b137386d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_lr = tng_datasets['train'][0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19070849-0fa3-4817-bd78-6b948f5e89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_img(sample['lr'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e5e0d0-7f4c-4afc-84e5-86faa5f5ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e533dcc0-452c-4ca8-863d-1ba83ccd7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Upsample(scale_factor=2, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "637f49cb-6530-4be5-80be-7377c4824b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3144c2a-24bf-4cfe-8a6c-07532752f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m(torch.unsqueeze(sample_lr, axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0acf697-b01c-4eb0-9754-5e818eab8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(torch.unsqueeze(sample_lr, axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3e74120-36fa-4e7b-abda-ba94ac766776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Demonstrate the transformations\n",
    "# for i in range(6):\n",
    "#     sample = tng_datasets['train'][i]\n",
    "#     plot_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf866f9-5ee9-43a9-9c82-c33bef603556",
   "metadata": {},
   "source": [
    "### TODO: implement upsamplig comparison: https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fda01-3d7c-4b30-bacf-1070f8480af8",
   "metadata": {},
   "source": [
    "# Super Resolution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50936aa3-a26e-4974-8dbb-8ba29a2ebf09",
   "metadata": {},
   "source": [
    "Basic idea from: https://debuggercafe.com/image-super-resolution-using-deep-learning-and-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c607392-cb53-47ee-928b-d590057e908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision.models import vgg19\n",
    "import math\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.vgg19_54(img)\n",
    "\n",
    "\n",
    "class DenseResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, res_scale=0.2):\n",
    "        super(DenseResidualBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "        def block(in_features, non_linearity=True):\n",
    "            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n",
    "            if non_linearity:\n",
    "                layers += [nn.LeakyReLU()]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.b1 = block(in_features=1 * filters)\n",
    "        self.b2 = block(in_features=2 * filters)\n",
    "        self.b3 = block(in_features=3 * filters)\n",
    "        self.b4 = block(in_features=4 * filters)\n",
    "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
    "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        for block in self.blocks:\n",
    "            out = block(inputs)\n",
    "            inputs = torch.cat([inputs, out], 1)\n",
    "        return out.mul(self.res_scale) + x\n",
    "\n",
    "\n",
    "class ResidualInResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, filters, res_scale=0.2):\n",
    "        super(ResidualInResidualDenseBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.dense_blocks = nn.Sequential(\n",
    "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
    "\n",
    "\n",
    "class GeneratorRRDB(nn.Module):\n",
    "    def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2):\n",
    "        super(GeneratorRRDB, self).__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
    "        # Upsampling layers\n",
    "        upsample_layers = []\n",
    "        for _ in range(num_upsample):\n",
    "            upsample_layers += [\n",
    "                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsample_layers)\n",
    "        # Final output block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96098e14-8b8a-4722-a7e7-fa250c38926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "# parser.add_argument(\"--dataset_name\", type=str, default=\"img_align_celeba\", help=\"name of the dataset\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=4, help=\"size of the batches\")\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "# parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n",
    "# parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "# parser.add_argument(\"--hr_height\", type=int, default=256, help=\"high res. image height\")\n",
    "# parser.add_argument(\"--hr_width\", type=int, default=256, help=\"high res. image width\")\n",
    "# parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "# parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between saving image samples\")\n",
    "# parser.add_argument(\"--checkpoint_interval\", type=int, default=5000, help=\"batch interval between model checkpoints\")\n",
    "# parser.add_argument(\"--residual_blocks\", type=int, default=23, help=\"number of residual blocks in the generator\")\n",
    "# parser.add_argument(\"--warmup_batches\", type=int, default=500, help=\"number of batches with pixel-wise loss only\")\n",
    "# parser.add_argument(\"--lambda_adv\", type=float, default=5e-3, help=\"adversarial loss weight\")\n",
    "# parser.add_argument(\"--lambda_pixel\", type=float, default=1e-2, help=\"pixel-wise loss weight\")\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    epoch = 0,\n",
    "    n_epochs = 5000,\n",
    "    channels = 1,\n",
    "    residual_blocks = 23,\n",
    "    project = \"esr_gan\",\n",
    "    root_dir = \"/home/ssweere/data/sim/tng300_2048/fits\",\n",
    "    runs_dir = \"/home/ssweere/remote_home/data/runs\", # The run specific folder will be created automatically\n",
    "    lr_res = 32,\n",
    "    hr_res = 128,\n",
    "    batch_size = 64,\n",
    "    crop_p = [0.5],\n",
    "    # crop_p = [0.125, 0.25, 0.5] #This will random sample from multiple crop percentages\n",
    "    normalize = 'sqrt', #Other options None and 'linear'\n",
    "    # normalize = None\n",
    "    preload = True, #Warning this increases the ram usage dramatically\n",
    "    sample_interval = 10,\n",
    "    lr = 0.0002,\n",
    "    b1= 0.9,\n",
    "    b2 = 0.999,\n",
    "    warmup_batches = 500,\n",
    "    lambda_adv = 5e-3,\n",
    "    lambda_pixel = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e106a148-47df-44ab-8347-d231376c8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=hyperparameters['project'], config=hyperparameters) as run:\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "        \n",
    "        # make the model, data, and optimization problem\n",
    "        generator, discriminator, feature_extractor, dataloaders, criterion_GAN, criterion_content, criterion_pixel , optimizer_G, optimizer_D, Tensor, run_path = make(config)\n",
    "        \n",
    "#         print(\"Generator:\")\n",
    "#         print(generator)\n",
    "        \n",
    "#         print(\"Discriminator:\")\n",
    "#         print(discriminator)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(generator, discriminator, feature_extractor, dataloaders, criterion_GAN, criterion_content, criterion_pixel , optimizer_G, optimizer_D, Tensor, run_path, config)\n",
    "\n",
    "        #       # and test its final performance\n",
    "        #       test(model, test_loader)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4f2e066-3479-475c-ae03-3acff1138e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Super-resolution of CelebA using Generative Adversarial Networks.\n",
    "The dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\n",
    "(if not available there see if options are listed at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n",
    "Instrustion on running the script:\n",
    "1. Download the dataset from the provided link\n",
    "2. Save the folder 'img_align_celeba' to '../../data/'\n",
    "4. Run the sript using command 'python3 esrgan.py'\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def make(config):\n",
    "    # Make the data\n",
    "    # Prepare the data transforms\n",
    "    data_transform = transform=transforms.Compose([\n",
    "                                            RandomCrop(crop_p = config.crop_p),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomVerticalFlip(),\n",
    "                                        ])\n",
    "\n",
    "    tng_dataset = TngDataset(root_dir = config.root_dir, lr_res = config.lr_res, hr_res = config.hr_res, normalize=config.normalize, transform=data_transform, preload=config.preload)\n",
    "\n",
    "    train_val_test_split = [0.7, 0.15, 0.15]\n",
    "    train_len = int(len(tng_dataset)*train_val_test_split[0])\n",
    "    val_len = int(len(tng_dataset)*train_val_test_split[1])\n",
    "    test_len = len(tng_dataset) - train_len - val_len\n",
    "\n",
    "    # Note that the test set has the same transformations as the train an validation set\n",
    "    tng_datasets = torch.utils.data.random_split(tng_dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "    tng_datasets = {'train':tng_datasets[0], 'val':tng_datasets[1], 'test':tng_datasets[2]}\n",
    "\n",
    "    print(\"Train set size:\", len(tng_datasets['train']))\n",
    "    print(\"Validation set size:\", len(tng_datasets['val']))\n",
    "    print(\"Test set size:\", len(tng_datasets['test']))\n",
    "\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(tng_datasets[x], batch_size=config.batch_size,\n",
    "                                                 shuffle=True, num_workers=0)\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "\n",
    "    # Create the checkpoint and output folder\n",
    "    run_name = wandb.run.name\n",
    "    # Create the run folder by combining the runs dir, project name and the run name\n",
    "    run_path = os.path.join(config.runs_dir, config.project, run_name)\n",
    "    if not os.path.exists(run_path):\n",
    "        os.makedirs(run_path)\n",
    "    print(\"Run path:\", run_path)\n",
    "\n",
    "    # Also create the checkpoints and figures folder\n",
    "    checkpoint_path = os.path.join(run_path, \"checkpoints\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "        \n",
    "    figs_path = os.path.join(run_path, \"figures\")\n",
    "    if not os.path.exists(figs_path):\n",
    "        os.makedirs(figs_path)\n",
    "\n",
    "   \n",
    "    # Make the model\n",
    "    hr_shape = (config.hr_res, config.hr_res)\n",
    "    \n",
    "    # Initialize generator and discriminator\n",
    "    generator = GeneratorRRDB(config.channels, filters=64, num_res_blocks=config.residual_blocks).to(device)\n",
    "    discriminator = Discriminator(input_shape=(config.channels, *hr_shape)).to(device)\n",
    "    feature_extractor = FeatureExtractor().to(device)\n",
    "        \n",
    "    # Set feature extractor to inference mode\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    # Losses\n",
    "    criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "    criterion_content = torch.nn.L1Loss().to(device)\n",
    "    criterion_pixel = torch.nn.L1Loss().to(device)\n",
    "\n",
    "    if config.epoch != 0:\n",
    "        # Load pretrained models\n",
    "        generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\" % config.epoch))\n",
    "        discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\" % config.epoch))\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=config.lr, betas=(config.b1, config.b2))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=config.lr, betas=(config.b1, config.b2))\n",
    "\n",
    "    Tensor = torch.Tensor #TODO: torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        \n",
    "#     # Make the model\n",
    "#     model = SRCNN().to(device)\n",
    "\n",
    "#     # Make the loss and optimizer\n",
    "#     # optimizer\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     # loss function \n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "    return generator, discriminator, feature_extractor, dataloaders, criterion_GAN, criterion_content, criterion_pixel , optimizer_G, optimizer_D, Tensor, run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "955fa499-0e16-4003-895a-da5b714282bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, feature_extractor, dataloaders, criterion_GAN, criterion_content, criterion_pixel, \n",
    "          optimizer_G, optimizer_D, Tensor, run_path, config):\n",
    "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    # This is inspired on: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=bZiTlrNkRKzm\n",
    "    wandb.watch(generator, log=\"all\", log_freq=10)\n",
    "    \n",
    "    wandb.watch(discriminator, log=\"all\", log_freq=10)\n",
    "    \n",
    "    \n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "    dataloader = dataloaders['train']\n",
    "\n",
    "    for epoch in range(config.epoch, config.n_epochs):\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "            # Configure model input\n",
    "            imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
    "            imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a high resolution image from low resolution input\n",
    "            gen_hr = generator(imgs_lr)\n",
    "\n",
    "            # Measure pixel-wise loss against ground truth\n",
    "            loss_pixel = criterion_pixel(gen_hr, imgs_hr)\n",
    "\n",
    "            if batches_done < config.warmup_batches:\n",
    "                # Warm-up (pixel-wise loss only)\n",
    "                loss_pixel.backward()\n",
    "                optimizer_G.step()\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
    "                    % (epoch, config.n_epochs, i, len(dataloader), loss_pixel.item())\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Extract validity predictions from discriminator\n",
    "            pred_real = discriminator(imgs_hr).detach()\n",
    "            pred_fake = discriminator(gen_hr)\n",
    "\n",
    "            # Adversarial loss (relativistic average GAN)\n",
    "            loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "            # Content loss\n",
    "            gen_features = feature_extractor(gen_hr)\n",
    "            real_features = feature_extractor(imgs_hr).detach()\n",
    "            loss_content = criterion_content(gen_features, real_features)\n",
    "\n",
    "            # Total generator loss\n",
    "            loss_G = loss_content + config.lambda_adv * loss_GAN +configopt.lambda_pixel * loss_pixel\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            pred_real = discriminator(imgs_hr)\n",
    "            pred_fake = discriminator(gen_hr.detach())\n",
    "\n",
    "            # Adversarial loss for real and fake images (relativistic average GAN)\n",
    "            loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "            loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, content: %f, adv: %f, pixel: %f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    config.n_epochs,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_content.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    loss_pixel.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\"epoch\": epoch, \"batch\":batches_done, \"loss_D\": loss_D.item(), \"loss_G\": loss_G.item(), \n",
    "                       \"loss_content\": loss_content.item(), \"loss_GAN\": loss_GAN.item(), \"loss_pixel\": loss_pixel.item()})\n",
    "\n",
    "\n",
    "\n",
    "            if batches_done % config.sample_interval == 0:\n",
    "                # Save image grid with upsampled inputs and ESRGAN outputs\n",
    "                imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
    "                img_grid = denormalize(torch.cat((imgs_lr, gen_hr), -1))\n",
    "\n",
    "                images = wandb.Image(img_grid)\n",
    "                wandb.log({\"img_grid\": images})\n",
    "                          \n",
    "                images = []\n",
    "                for i in range(len(imgs_lr)):\n",
    "                    images += [imgs_lr[i][0], gen_hr[i][0], imgs_hr[i][0]]\n",
    "\n",
    "                # TODO: do this in the plot img function\n",
    "                #                     lr = np.repeat(image_lr[i][0], int(image_hr.shape[-1]/image_lr.shape[-1]), axis=0)\n",
    "                #                     lr = np.repeat(lr, int(image_hr.shape[-1]/image_lr.shape[-1]), axis=1)\n",
    "\n",
    "\n",
    "                #                 titles += ['Input','Generated', \"Target\"]\n",
    "                #                 plot_multiple_img([image_lr[i][0], outputs[i][0], image_hr[i][0]], titles = ['Input','Generated', \"Target\"])\n",
    "\n",
    "                plot_multiple_img(images, titles = ['Input','Generated', \"Label\"], cols=3, merge=True, colorbar=False, figsize=(3*5, len(outputs)*5), save_fig=save_results, show_plot=show_results, wandb_log=True)\n",
    "            \n",
    "\n",
    "    #             save_image(img_grid, os.path.join(run_path, \"figures\",f\"training_{batches_done}.png\"), nrow=1, normalize=False)\n",
    "\n",
    "            if batches_done % config.checkpoint_interval == 0:\n",
    "                # Save model checkpoints\n",
    "                torch.save(generator.state_dict(), os.path.join(run_path, \"checkpoints\" , f\"generator_{epoch}.pt\"))\n",
    "                torch.save(discriminator.state_dict(), os.path.join(run_path, \"checkpoints\" , \"discriminator_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b0587e-8428-4d33-b52b-ed5e90c305bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sage-flower-20</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/samsweere/esr_gan\" target=\"_blank\">https://wandb.ai/samsweere/esr_gan</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/samsweere/esr_gan/runs/jptn01bq\" target=\"_blank\">https://wandb.ai/samsweere/esr_gan/runs/jptn01bq</a><br/>\n",
       "                Run data is saved locally in <code>/home/ssweere/xmm-superres/sr/notebooks/wandb/run-20210513_174913-jptn01bq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1200 [00:00<00:43, 27.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading the fits images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1200/1200 [00:33<00:00, 35.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 840\n",
      "Validation set size: 180\n",
      "Test set size: 180\n",
      "Run path: /home/ssweere/remote_home/data/runs/esr_gan/sage-flower-20\n",
      "[Epoch 0/5000] [Batch 0/14] [G pixel: 0.236737]\n",
      "[Epoch 0/5000] [Batch 1/14] [G pixel: 0.211381]\n",
      "[Epoch 0/5000] [Batch 4/14] [G pixel: 0.121034]\n",
      "[Epoch 0/5000] [Batch 5/14] [G pixel: 0.118620]\n",
      "[Epoch 0/5000] [Batch 6/14] [G pixel: 0.084931]\n",
      "[Epoch 0/5000] [Batch 7/14] [G pixel: 0.073117]\n",
      "[Epoch 0/5000] [Batch 8/14] [G pixel: 0.075345]\n",
      "[Epoch 0/5000] [Batch 9/14] [G pixel: 0.067249]\n",
      "[Epoch 0/5000] [Batch 10/14] [G pixel: 0.062188]\n",
      "[Epoch 0/5000] [Batch 11/14] [G pixel: 0.063626]\n",
      "[Epoch 0/5000] [Batch 12/14] [G pixel: 0.064401]\n",
      "[Epoch 0/5000] [Batch 13/14] [G pixel: 0.057113]\n",
      "[Epoch 1/5000] [Batch 0/14] [G pixel: 0.057759]\n",
      "[Epoch 1/5000] [Batch 1/14] [G pixel: 0.065911]\n",
      "[Epoch 1/5000] [Batch 2/14] [G pixel: 0.058643]\n",
      "[Epoch 1/5000] [Batch 3/14] [G pixel: 0.055523]\n",
      "[Epoch 1/5000] [Batch 4/14] [G pixel: 0.054242]\n",
      "[Epoch 1/5000] [Batch 5/14] [G pixel: 0.054791]\n",
      "[Epoch 1/5000] [Batch 6/14] [G pixel: 0.056119]\n",
      "[Epoch 1/5000] [Batch 7/14] [G pixel: 0.051785]\n",
      "[Epoch 1/5000] [Batch 8/14] [G pixel: 0.057399]\n",
      "[Epoch 1/5000] [Batch 9/14] [G pixel: 0.047759]\n",
      "[Epoch 1/5000] [Batch 10/14] [G pixel: 0.047813]\n",
      "[Epoch 1/5000] [Batch 11/14] [G pixel: 0.043825]\n",
      "[Epoch 1/5000] [Batch 12/14] [G pixel: 0.052511]\n",
      "[Epoch 1/5000] [Batch 13/14] [G pixel: 0.029376]\n",
      "[Epoch 2/5000] [Batch 0/14] [G pixel: 0.050922]\n",
      "[Epoch 2/5000] [Batch 1/14] [G pixel: 0.045261]\n",
      "[Epoch 2/5000] [Batch 2/14] [G pixel: 0.044092]\n",
      "[Epoch 2/5000] [Batch 3/14] [G pixel: 0.048158]\n",
      "[Epoch 2/5000] [Batch 4/14] [G pixel: 0.050934]\n",
      "[Epoch 2/5000] [Batch 5/14] [G pixel: 0.040500]\n",
      "[Epoch 2/5000] [Batch 6/14] [G pixel: 0.048836]\n",
      "[Epoch 2/5000] [Batch 7/14] [G pixel: 0.041078]\n",
      "[Epoch 2/5000] [Batch 8/14] [G pixel: 0.045621]\n",
      "[Epoch 2/5000] [Batch 9/14] [G pixel: 0.044872]\n",
      "[Epoch 2/5000] [Batch 10/14] [G pixel: 0.047440]\n",
      "[Epoch 2/5000] [Batch 11/14] [G pixel: 0.036787]\n",
      "[Epoch 2/5000] [Batch 12/14] [G pixel: 0.035344]\n",
      "[Epoch 2/5000] [Batch 13/14] [G pixel: 0.036761]\n",
      "[Epoch 3/5000] [Batch 0/14] [G pixel: 0.038123]\n",
      "[Epoch 3/5000] [Batch 1/14] [G pixel: 0.041359]\n",
      "[Epoch 3/5000] [Batch 2/14] [G pixel: 0.039478]\n",
      "[Epoch 3/5000] [Batch 3/14] [G pixel: 0.046825]\n",
      "[Epoch 3/5000] [Batch 4/14] [G pixel: 0.045749]\n",
      "[Epoch 3/5000] [Batch 5/14] [G pixel: 0.046809]\n",
      "[Epoch 3/5000] [Batch 6/14] [G pixel: 0.046307]\n",
      "[Epoch 3/5000] [Batch 7/14] [G pixel: 0.039517]\n",
      "[Epoch 3/5000] [Batch 8/14] [G pixel: 0.047652]\n",
      "[Epoch 3/5000] [Batch 9/14] [G pixel: 0.039405]\n",
      "[Epoch 3/5000] [Batch 10/14] [G pixel: 0.041522]\n",
      "[Epoch 3/5000] [Batch 11/14] [G pixel: 0.040346]\n",
      "[Epoch 3/5000] [Batch 12/14] [G pixel: 0.038627]\n",
      "[Epoch 3/5000] [Batch 13/14] [G pixel: 0.031747]\n",
      "[Epoch 4/5000] [Batch 0/14] [G pixel: 0.036739]\n",
      "[Epoch 4/5000] [Batch 1/14] [G pixel: 0.038476]\n",
      "[Epoch 4/5000] [Batch 2/14] [G pixel: 0.045321]\n",
      "[Epoch 4/5000] [Batch 3/14] [G pixel: 0.033712]\n",
      "[Epoch 4/5000] [Batch 4/14] [G pixel: 0.043289]\n",
      "[Epoch 4/5000] [Batch 5/14] [G pixel: 0.040988]\n",
      "[Epoch 4/5000] [Batch 6/14] [G pixel: 0.042496]\n",
      "[Epoch 4/5000] [Batch 7/14] [G pixel: 0.047116]\n",
      "[Epoch 4/5000] [Batch 8/14] [G pixel: 0.037605]\n",
      "[Epoch 4/5000] [Batch 9/14] [G pixel: 0.038779]\n",
      "[Epoch 4/5000] [Batch 10/14] [G pixel: 0.037261]\n",
      "[Epoch 4/5000] [Batch 11/14] [G pixel: 0.036070]\n",
      "[Epoch 4/5000] [Batch 12/14] [G pixel: 0.039828]\n",
      "[Epoch 4/5000] [Batch 13/14] [G pixel: 0.025627]\n",
      "[Epoch 5/5000] [Batch 0/14] [G pixel: 0.036993]\n",
      "[Epoch 5/5000] [Batch 1/14] [G pixel: 0.037915]\n",
      "[Epoch 5/5000] [Batch 2/14] [G pixel: 0.046969]\n",
      "[Epoch 5/5000] [Batch 3/14] [G pixel: 0.044480]\n",
      "[Epoch 5/5000] [Batch 4/14] [G pixel: 0.044486]\n",
      "[Epoch 5/5000] [Batch 5/14] [G pixel: 0.040163]\n",
      "[Epoch 5/5000] [Batch 6/14] [G pixel: 0.042158]\n",
      "[Epoch 5/5000] [Batch 7/14] [G pixel: 0.040496]\n",
      "[Epoch 5/5000] [Batch 8/14] [G pixel: 0.033795]\n",
      "[Epoch 5/5000] [Batch 9/14] [G pixel: 0.031018]\n",
      "[Epoch 5/5000] [Batch 10/14] [G pixel: 0.037952]\n",
      "[Epoch 5/5000] [Batch 11/14] [G pixel: 0.040521]\n",
      "[Epoch 5/5000] [Batch 12/14] [G pixel: 0.036985]\n",
      "[Epoch 5/5000] [Batch 13/14] [G pixel: 0.045153]\n",
      "[Epoch 6/5000] [Batch 0/14] [G pixel: 0.036774]\n",
      "[Epoch 6/5000] [Batch 1/14] [G pixel: 0.037255]\n",
      "[Epoch 6/5000] [Batch 2/14] [G pixel: 0.031951]\n",
      "[Epoch 6/5000] [Batch 3/14] [G pixel: 0.035542]\n",
      "[Epoch 6/5000] [Batch 4/14] [G pixel: 0.036347]\n",
      "[Epoch 6/5000] [Batch 5/14] [G pixel: 0.043288]\n",
      "[Epoch 6/5000] [Batch 6/14] [G pixel: 0.036735]\n",
      "[Epoch 6/5000] [Batch 7/14] [G pixel: 0.037910]\n",
      "[Epoch 6/5000] [Batch 8/14] [G pixel: 0.055143]\n",
      "[Epoch 6/5000] [Batch 9/14] [G pixel: 0.044618]\n",
      "[Epoch 6/5000] [Batch 10/14] [G pixel: 0.043555]\n",
      "[Epoch 6/5000] [Batch 11/14] [G pixel: 0.052226]\n",
      "[Epoch 6/5000] [Batch 12/14] [G pixel: 0.054490]\n",
      "[Epoch 6/5000] [Batch 13/14] [G pixel: 0.024255]\n",
      "[Epoch 7/5000] [Batch 0/14] [G pixel: 0.070170]\n",
      "[Epoch 7/5000] [Batch 1/14] [G pixel: 0.029232]\n",
      "[Epoch 7/5000] [Batch 2/14] [G pixel: 0.049772]\n",
      "[Epoch 7/5000] [Batch 3/14] [G pixel: 0.054530]\n",
      "[Epoch 7/5000] [Batch 4/14] [G pixel: 0.039672]\n",
      "[Epoch 7/5000] [Batch 5/14] [G pixel: 0.042971]\n",
      "[Epoch 7/5000] [Batch 6/14] [G pixel: 0.055570]\n",
      "[Epoch 7/5000] [Batch 7/14] [G pixel: 0.046260]\n",
      "[Epoch 7/5000] [Batch 8/14] [G pixel: 0.052707]\n",
      "[Epoch 7/5000] [Batch 9/14] [G pixel: 0.047238]\n",
      "[Epoch 7/5000] [Batch 10/14] [G pixel: 0.047800]\n",
      "[Epoch 7/5000] [Batch 11/14] [G pixel: 0.041154]\n",
      "[Epoch 7/5000] [Batch 12/14] [G pixel: 0.041214]\n",
      "[Epoch 7/5000] [Batch 13/14] [G pixel: 0.050389]\n",
      "[Epoch 8/5000] [Batch 0/14] [G pixel: 0.044870]\n",
      "[Epoch 8/5000] [Batch 1/14] [G pixel: 0.042906]\n",
      "[Epoch 8/5000] [Batch 2/14] [G pixel: 0.044375]\n",
      "[Epoch 8/5000] [Batch 3/14] [G pixel: 0.047448]\n",
      "[Epoch 8/5000] [Batch 4/14] [G pixel: 0.040267]\n",
      "[Epoch 8/5000] [Batch 5/14] [G pixel: 0.040221]\n",
      "[Epoch 8/5000] [Batch 6/14] [G pixel: 0.044083]\n",
      "[Epoch 8/5000] [Batch 7/14] [G pixel: 0.044021]\n",
      "[Epoch 8/5000] [Batch 8/14] [G pixel: 0.039281]\n",
      "[Epoch 8/5000] [Batch 9/14] [G pixel: 0.038799]\n",
      "[Epoch 8/5000] [Batch 10/14] [G pixel: 0.041196]\n",
      "[Epoch 8/5000] [Batch 11/14] [G pixel: 0.035042]\n",
      "[Epoch 8/5000] [Batch 12/14] [G pixel: 0.040228]\n",
      "[Epoch 8/5000] [Batch 13/14] [G pixel: 0.035917]\n",
      "[Epoch 9/5000] [Batch 0/14] [G pixel: 0.042243]\n",
      "[Epoch 9/5000] [Batch 1/14] [G pixel: 0.037761]\n",
      "[Epoch 9/5000] [Batch 2/14] [G pixel: 0.036147]\n",
      "[Epoch 9/5000] [Batch 3/14] [G pixel: 0.038642]\n",
      "[Epoch 9/5000] [Batch 4/14] [G pixel: 0.039000]\n",
      "[Epoch 9/5000] [Batch 5/14] [G pixel: 0.038186]\n",
      "[Epoch 9/5000] [Batch 6/14] [G pixel: 0.035003]\n",
      "[Epoch 9/5000] [Batch 7/14] [G pixel: 0.040938]\n",
      "[Epoch 9/5000] [Batch 8/14] [G pixel: 0.043819]\n",
      "[Epoch 9/5000] [Batch 9/14] [G pixel: 0.040721]\n",
      "[Epoch 9/5000] [Batch 10/14] [G pixel: 0.040200]\n",
      "[Epoch 9/5000] [Batch 11/14] [G pixel: 0.032327]\n",
      "[Epoch 9/5000] [Batch 12/14] [G pixel: 0.043719]\n",
      "[Epoch 9/5000] [Batch 13/14] [G pixel: 0.029994]\n",
      "[Epoch 10/5000] [Batch 0/14] [G pixel: 0.036523]\n",
      "[Epoch 10/5000] [Batch 1/14] [G pixel: 0.040477]\n",
      "[Epoch 10/5000] [Batch 2/14] [G pixel: 0.039477]\n",
      "[Epoch 10/5000] [Batch 3/14] [G pixel: 0.039176]\n",
      "[Epoch 10/5000] [Batch 4/14] [G pixel: 0.035864]\n",
      "[Epoch 10/5000] [Batch 5/14] [G pixel: 0.042357]\n",
      "[Epoch 10/5000] [Batch 6/14] [G pixel: 0.041382]\n",
      "[Epoch 10/5000] [Batch 7/14] [G pixel: 0.036877]\n",
      "[Epoch 10/5000] [Batch 8/14] [G pixel: 0.033208]\n",
      "[Epoch 10/5000] [Batch 9/14] [G pixel: 0.040454]\n",
      "[Epoch 10/5000] [Batch 10/14] [G pixel: 0.043236]\n",
      "[Epoch 10/5000] [Batch 11/14] [G pixel: 0.044086]\n",
      "[Epoch 10/5000] [Batch 12/14] [G pixel: 0.048312]\n",
      "[Epoch 10/5000] [Batch 13/14] [G pixel: 0.040467]\n",
      "[Epoch 11/5000] [Batch 0/14] [G pixel: 0.047042]\n",
      "[Epoch 11/5000] [Batch 1/14] [G pixel: 0.044983]\n",
      "[Epoch 11/5000] [Batch 2/14] [G pixel: 0.034557]\n",
      "[Epoch 11/5000] [Batch 3/14] [G pixel: 0.043146]\n",
      "[Epoch 11/5000] [Batch 4/14] [G pixel: 0.044816]\n",
      "[Epoch 11/5000] [Batch 5/14] [G pixel: 0.036692]\n",
      "[Epoch 11/5000] [Batch 6/14] [G pixel: 0.038974]\n",
      "[Epoch 11/5000] [Batch 7/14] [G pixel: 0.037747]\n",
      "[Epoch 11/5000] [Batch 8/14] [G pixel: 0.037621]\n",
      "[Epoch 11/5000] [Batch 9/14] [G pixel: 0.035809]\n",
      "[Epoch 11/5000] [Batch 10/14] [G pixel: 0.040381]\n",
      "[Epoch 11/5000] [Batch 11/14] [G pixel: 0.031844]\n",
      "[Epoch 11/5000] [Batch 12/14] [G pixel: 0.041374]\n",
      "[Epoch 11/5000] [Batch 13/14] [G pixel: 0.039867]\n",
      "[Epoch 12/5000] [Batch 0/14] [G pixel: 0.036225]\n",
      "[Epoch 12/5000] [Batch 1/14] [G pixel: 0.038252]\n",
      "[Epoch 12/5000] [Batch 2/14] [G pixel: 0.047292]\n",
      "[Epoch 12/5000] [Batch 3/14] [G pixel: 0.039509]\n",
      "[Epoch 12/5000] [Batch 4/14] [G pixel: 0.041138]\n",
      "[Epoch 12/5000] [Batch 5/14] [G pixel: 0.040034]\n",
      "[Epoch 12/5000] [Batch 6/14] [G pixel: 0.044569]\n",
      "[Epoch 12/5000] [Batch 7/14] [G pixel: 0.039935]\n",
      "[Epoch 12/5000] [Batch 8/14] [G pixel: 0.038823]\n",
      "[Epoch 12/5000] [Batch 9/14] [G pixel: 0.041333]\n",
      "[Epoch 12/5000] [Batch 10/14] [G pixel: 0.035903]\n",
      "[Epoch 12/5000] [Batch 11/14] [G pixel: 0.040442]\n",
      "[Epoch 12/5000] [Batch 12/14] [G pixel: 0.036812]\n",
      "[Epoch 12/5000] [Batch 13/14] [G pixel: 0.054278]\n",
      "[Epoch 13/5000] [Batch 0/14] [G pixel: 0.048134]\n",
      "[Epoch 13/5000] [Batch 1/14] [G pixel: 0.055781]\n",
      "[Epoch 13/5000] [Batch 2/14] [G pixel: 0.043859]\n",
      "[Epoch 13/5000] [Batch 3/14] [G pixel: 0.036380]\n",
      "[Epoch 13/5000] [Batch 4/14] [G pixel: 0.055146]\n",
      "[Epoch 13/5000] [Batch 5/14] [G pixel: 0.041844]\n",
      "[Epoch 13/5000] [Batch 6/14] [G pixel: 0.041569]\n",
      "[Epoch 13/5000] [Batch 7/14] [G pixel: 0.046193]\n",
      "[Epoch 13/5000] [Batch 8/14] [G pixel: 0.050972]\n",
      "[Epoch 13/5000] [Batch 9/14] [G pixel: 0.042777]\n",
      "[Epoch 13/5000] [Batch 10/14] [G pixel: 0.036945]\n",
      "[Epoch 13/5000] [Batch 11/14] [G pixel: 0.041300]\n",
      "[Epoch 13/5000] [Batch 12/14] [G pixel: 0.034332]\n",
      "[Epoch 13/5000] [Batch 13/14] [G pixel: 0.024538]\n",
      "[Epoch 14/5000] [Batch 0/14] [G pixel: 0.041106]\n",
      "[Epoch 14/5000] [Batch 1/14] [G pixel: 0.038903]\n",
      "[Epoch 14/5000] [Batch 2/14] [G pixel: 0.033721]\n",
      "[Epoch 14/5000] [Batch 3/14] [G pixel: 0.036320]\n",
      "[Epoch 14/5000] [Batch 4/14] [G pixel: 0.035456]\n",
      "[Epoch 14/5000] [Batch 5/14] [G pixel: 0.039763]\n",
      "[Epoch 14/5000] [Batch 6/14] [G pixel: 0.038827]\n",
      "[Epoch 14/5000] [Batch 7/14] [G pixel: 0.044915]\n",
      "[Epoch 14/5000] [Batch 8/14] [G pixel: 0.042624]\n",
      "[Epoch 14/5000] [Batch 9/14] [G pixel: 0.037341]\n",
      "[Epoch 14/5000] [Batch 10/14] [G pixel: 0.035245]\n",
      "[Epoch 14/5000] [Batch 11/14] [G pixel: 0.053899]\n",
      "[Epoch 14/5000] [Batch 12/14] [G pixel: 0.042434]\n",
      "[Epoch 14/5000] [Batch 13/14] [G pixel: 0.063374]\n",
      "[Epoch 15/5000] [Batch 0/14] [G pixel: 0.041894]\n",
      "[Epoch 15/5000] [Batch 1/14] [G pixel: 0.033312]\n",
      "[Epoch 15/5000] [Batch 2/14] [G pixel: 0.050584]\n",
      "[Epoch 15/5000] [Batch 3/14] [G pixel: 0.042200]\n",
      "[Epoch 15/5000] [Batch 4/14] [G pixel: 0.037948]\n",
      "[Epoch 15/5000] [Batch 5/14] [G pixel: 0.041880]\n",
      "[Epoch 15/5000] [Batch 6/14] [G pixel: 0.040555]\n",
      "[Epoch 15/5000] [Batch 7/14] [G pixel: 0.033522]\n",
      "[Epoch 15/5000] [Batch 8/14] [G pixel: 0.038308]\n",
      "[Epoch 15/5000] [Batch 9/14] [G pixel: 0.039561]\n",
      "[Epoch 15/5000] [Batch 10/14] [G pixel: 0.039441]\n",
      "[Epoch 15/5000] [Batch 11/14] [G pixel: 0.036342]\n",
      "[Epoch 15/5000] [Batch 12/14] [G pixel: 0.037912]\n",
      "[Epoch 15/5000] [Batch 13/14] [G pixel: 0.047014]\n",
      "[Epoch 16/5000] [Batch 0/14] [G pixel: 0.039919]\n",
      "[Epoch 16/5000] [Batch 1/14] [G pixel: 0.034541]\n",
      "[Epoch 16/5000] [Batch 2/14] [G pixel: 0.035704]\n",
      "[Epoch 16/5000] [Batch 3/14] [G pixel: 0.038502]\n",
      "[Epoch 16/5000] [Batch 4/14] [G pixel: 0.035072]\n",
      "[Epoch 16/5000] [Batch 5/14] [G pixel: 0.037079]\n",
      "[Epoch 16/5000] [Batch 6/14] [G pixel: 0.042793]\n",
      "[Epoch 16/5000] [Batch 7/14] [G pixel: 0.033468]\n",
      "[Epoch 16/5000] [Batch 8/14] [G pixel: 0.041194]\n",
      "[Epoch 16/5000] [Batch 9/14] [G pixel: 0.037505]\n",
      "[Epoch 16/5000] [Batch 10/14] [G pixel: 0.045278]\n",
      "[Epoch 16/5000] [Batch 11/14] [G pixel: 0.044690]\n",
      "[Epoch 16/5000] [Batch 12/14] [G pixel: 0.038537]\n",
      "[Epoch 16/5000] [Batch 13/14] [G pixel: 0.028052]\n",
      "[Epoch 17/5000] [Batch 0/14] [G pixel: 0.040634]\n",
      "[Epoch 17/5000] [Batch 1/14] [G pixel: 0.035740]\n",
      "[Epoch 17/5000] [Batch 2/14] [G pixel: 0.040968]\n",
      "[Epoch 17/5000] [Batch 3/14] [G pixel: 0.042171]\n",
      "[Epoch 17/5000] [Batch 4/14] [G pixel: 0.035764]\n",
      "[Epoch 17/5000] [Batch 5/14] [G pixel: 0.036811]\n",
      "[Epoch 17/5000] [Batch 6/14] [G pixel: 0.036972]\n",
      "[Epoch 17/5000] [Batch 7/14] [G pixel: 0.037586]\n",
      "[Epoch 17/5000] [Batch 8/14] [G pixel: 0.037742]\n",
      "[Epoch 17/5000] [Batch 9/14] [G pixel: 0.038779]\n",
      "[Epoch 17/5000] [Batch 10/14] [G pixel: 0.032674]\n",
      "[Epoch 17/5000] [Batch 11/14] [G pixel: 0.036241]\n",
      "[Epoch 17/5000] [Batch 12/14] [G pixel: 0.039801]\n",
      "[Epoch 17/5000] [Batch 13/14] [G pixel: 0.049632]\n",
      "[Epoch 18/5000] [Batch 0/14] [G pixel: 0.042743]\n",
      "[Epoch 18/5000] [Batch 1/14] [G pixel: 0.046328]\n",
      "[Epoch 18/5000] [Batch 2/14] [G pixel: 0.052601]\n",
      "[Epoch 18/5000] [Batch 3/14] [G pixel: 0.036968]\n",
      "[Epoch 18/5000] [Batch 4/14] [G pixel: 0.048563]\n",
      "[Epoch 18/5000] [Batch 5/14] [G pixel: 0.044361]\n",
      "[Epoch 18/5000] [Batch 6/14] [G pixel: 0.045329]\n",
      "[Epoch 18/5000] [Batch 7/14] [G pixel: 0.047072]\n",
      "[Epoch 18/5000] [Batch 8/14] [G pixel: 0.052477]\n",
      "[Epoch 18/5000] [Batch 9/14] [G pixel: 0.044336]\n",
      "[Epoch 18/5000] [Batch 10/14] [G pixel: 0.049415]\n",
      "[Epoch 18/5000] [Batch 11/14] [G pixel: 0.041996]\n",
      "[Epoch 18/5000] [Batch 12/14] [G pixel: 0.040967]\n",
      "[Epoch 18/5000] [Batch 13/14] [G pixel: 0.045742]\n",
      "[Epoch 19/5000] [Batch 0/14] [G pixel: 0.037879]\n",
      "[Epoch 19/5000] [Batch 1/14] [G pixel: 0.039868]\n",
      "[Epoch 19/5000] [Batch 2/14] [G pixel: 0.043197]\n",
      "[Epoch 19/5000] [Batch 3/14] [G pixel: 0.032347]\n",
      "[Epoch 19/5000] [Batch 4/14] [G pixel: 0.036229]\n",
      "[Epoch 19/5000] [Batch 5/14] [G pixel: 0.038870]\n",
      "[Epoch 19/5000] [Batch 6/14] [G pixel: 0.038698]\n",
      "[Epoch 19/5000] [Batch 7/14] [G pixel: 0.040192]\n",
      "[Epoch 19/5000] [Batch 8/14] [G pixel: 0.040387]\n",
      "[Epoch 19/5000] [Batch 9/14] [G pixel: 0.036830]\n",
      "[Epoch 19/5000] [Batch 10/14] [G pixel: 0.041944]\n",
      "[Epoch 19/5000] [Batch 11/14] [G pixel: 0.041258]\n",
      "[Epoch 19/5000] [Batch 12/14] [G pixel: 0.040500]\n",
      "[Epoch 19/5000] [Batch 13/14] [G pixel: 0.041563]\n",
      "[Epoch 20/5000] [Batch 0/14] [G pixel: 0.042145]\n",
      "[Epoch 20/5000] [Batch 1/14] [G pixel: 0.038217]\n",
      "[Epoch 20/5000] [Batch 2/14] [G pixel: 0.041426]\n",
      "[Epoch 20/5000] [Batch 3/14] [G pixel: 0.045100]\n",
      "[Epoch 20/5000] [Batch 4/14] [G pixel: 0.043185]\n",
      "[Epoch 20/5000] [Batch 5/14] [G pixel: 0.038127]\n",
      "[Epoch 20/5000] [Batch 6/14] [G pixel: 0.043385]\n",
      "[Epoch 20/5000] [Batch 7/14] [G pixel: 0.041297]\n",
      "[Epoch 20/5000] [Batch 8/14] [G pixel: 0.035834]\n",
      "[Epoch 20/5000] [Batch 9/14] [G pixel: 0.037850]\n",
      "[Epoch 20/5000] [Batch 10/14] [G pixel: 0.038869]\n",
      "[Epoch 20/5000] [Batch 11/14] [G pixel: 0.040876]\n",
      "[Epoch 20/5000] [Batch 12/14] [G pixel: 0.037571]\n",
      "[Epoch 20/5000] [Batch 13/14] [G pixel: 0.040274]\n",
      "[Epoch 21/5000] [Batch 0/14] [G pixel: 0.045261]\n",
      "[Epoch 21/5000] [Batch 1/14] [G pixel: 0.041520]\n",
      "[Epoch 21/5000] [Batch 2/14] [G pixel: 0.042798]\n",
      "[Epoch 21/5000] [Batch 3/14] [G pixel: 0.038135]\n",
      "[Epoch 21/5000] [Batch 4/14] [G pixel: 0.036161]\n",
      "[Epoch 21/5000] [Batch 5/14] [G pixel: 0.033558]\n",
      "[Epoch 21/5000] [Batch 6/14] [G pixel: 0.042259]\n",
      "[Epoch 21/5000] [Batch 7/14] [G pixel: 0.041230]\n",
      "[Epoch 21/5000] [Batch 8/14] [G pixel: 0.039087]\n",
      "[Epoch 21/5000] [Batch 9/14] [G pixel: 0.034531]\n",
      "[Epoch 21/5000] [Batch 10/14] [G pixel: 0.044191]\n",
      "[Epoch 21/5000] [Batch 11/14] [G pixel: 0.033667]\n",
      "[Epoch 21/5000] [Batch 12/14] [G pixel: 0.042714]\n",
      "[Epoch 21/5000] [Batch 13/14] [G pixel: 0.039692]\n",
      "[Epoch 22/5000] [Batch 0/14] [G pixel: 0.037833]\n",
      "[Epoch 22/5000] [Batch 1/14] [G pixel: 0.044720]\n",
      "[Epoch 22/5000] [Batch 2/14] [G pixel: 0.033743]\n",
      "[Epoch 22/5000] [Batch 3/14] [G pixel: 0.033807]\n",
      "[Epoch 22/5000] [Batch 4/14] [G pixel: 0.038087]\n",
      "[Epoch 22/5000] [Batch 5/14] [G pixel: 0.040836]\n",
      "[Epoch 22/5000] [Batch 6/14] [G pixel: 0.034273]\n",
      "[Epoch 22/5000] [Batch 7/14] [G pixel: 0.043793]\n",
      "[Epoch 22/5000] [Batch 8/14] [G pixel: 0.036424]\n",
      "[Epoch 22/5000] [Batch 9/14] [G pixel: 0.040757]\n",
      "[Epoch 22/5000] [Batch 10/14] [G pixel: 0.037782]\n",
      "[Epoch 22/5000] [Batch 11/14] [G pixel: 0.035921]\n",
      "[Epoch 22/5000] [Batch 12/14] [G pixel: 0.035148]\n",
      "[Epoch 22/5000] [Batch 13/14] [G pixel: 0.044174]\n",
      "[Epoch 23/5000] [Batch 0/14] [G pixel: 0.037951]\n",
      "[Epoch 23/5000] [Batch 1/14] [G pixel: 0.043086]\n",
      "[Epoch 23/5000] [Batch 2/14] [G pixel: 0.039953]\n",
      "[Epoch 23/5000] [Batch 3/14] [G pixel: 0.041048]\n",
      "[Epoch 23/5000] [Batch 4/14] [G pixel: 0.039261]\n",
      "[Epoch 23/5000] [Batch 5/14] [G pixel: 0.034559]\n",
      "[Epoch 23/5000] [Batch 6/14] [G pixel: 0.043899]\n",
      "[Epoch 23/5000] [Batch 7/14] [G pixel: 0.038980]\n",
      "[Epoch 23/5000] [Batch 8/14] [G pixel: 0.032190]\n",
      "[Epoch 23/5000] [Batch 9/14] [G pixel: 0.032317]\n",
      "[Epoch 23/5000] [Batch 10/14] [G pixel: 0.032961]\n",
      "[Epoch 23/5000] [Batch 11/14] [G pixel: 0.038427]\n",
      "[Epoch 23/5000] [Batch 12/14] [G pixel: 0.036817]\n",
      "[Epoch 23/5000] [Batch 13/14] [G pixel: 0.045443]\n",
      "[Epoch 24/5000] [Batch 0/14] [G pixel: 0.042062]\n",
      "[Epoch 24/5000] [Batch 1/14] [G pixel: 0.040707]\n",
      "[Epoch 24/5000] [Batch 2/14] [G pixel: 0.036811]\n",
      "[Epoch 24/5000] [Batch 3/14] [G pixel: 0.036138]\n",
      "[Epoch 24/5000] [Batch 4/14] [G pixel: 0.036853]\n",
      "[Epoch 24/5000] [Batch 5/14] [G pixel: 0.032175]\n",
      "[Epoch 24/5000] [Batch 6/14] [G pixel: 0.041539]\n",
      "[Epoch 24/5000] [Batch 7/14] [G pixel: 0.044942]\n",
      "[Epoch 24/5000] [Batch 8/14] [G pixel: 0.041941]\n",
      "[Epoch 24/5000] [Batch 9/14] [G pixel: 0.034448]\n",
      "[Epoch 24/5000] [Batch 10/14] [G pixel: 0.033210]\n",
      "[Epoch 24/5000] [Batch 11/14] [G pixel: 0.046681]\n",
      "[Epoch 24/5000] [Batch 12/14] [G pixel: 0.030942]\n",
      "[Epoch 24/5000] [Batch 13/14] [G pixel: 0.024925]\n",
      "[Epoch 25/5000] [Batch 0/14] [G pixel: 0.038289]\n",
      "[Epoch 25/5000] [Batch 1/14] [G pixel: 0.035165]\n",
      "[Epoch 25/5000] [Batch 2/14] [G pixel: 0.038082]\n",
      "[Epoch 25/5000] [Batch 3/14] [G pixel: 0.032799]\n",
      "[Epoch 25/5000] [Batch 4/14] [G pixel: 0.037309]\n",
      "[Epoch 25/5000] [Batch 5/14] [G pixel: 0.038996]\n",
      "[Epoch 25/5000] [Batch 6/14] [G pixel: 0.040008]\n",
      "[Epoch 25/5000] [Batch 7/14] [G pixel: 0.036050]\n",
      "[Epoch 25/5000] [Batch 8/14] [G pixel: 0.041161]\n",
      "[Epoch 25/5000] [Batch 9/14] [G pixel: 0.031831]\n",
      "[Epoch 25/5000] [Batch 10/14] [G pixel: 0.034782]\n",
      "[Epoch 25/5000] [Batch 11/14] [G pixel: 0.040811]\n",
      "[Epoch 25/5000] [Batch 12/14] [G pixel: 0.038882]\n",
      "[Epoch 25/5000] [Batch 13/14] [G pixel: 0.052279]\n",
      "[Epoch 26/5000] [Batch 0/14] [G pixel: 0.031644]\n",
      "[Epoch 26/5000] [Batch 1/14] [G pixel: 0.036964]\n",
      "[Epoch 26/5000] [Batch 2/14] [G pixel: 0.032726]\n",
      "[Epoch 26/5000] [Batch 3/14] [G pixel: 0.036841]\n",
      "[Epoch 26/5000] [Batch 4/14] [G pixel: 0.032118]\n",
      "[Epoch 26/5000] [Batch 5/14] [G pixel: 0.041127]\n",
      "[Epoch 26/5000] [Batch 6/14] [G pixel: 0.041806]\n",
      "[Epoch 26/5000] [Batch 7/14] [G pixel: 0.039529]\n",
      "[Epoch 26/5000] [Batch 8/14] [G pixel: 0.030673]\n",
      "[Epoch 26/5000] [Batch 9/14] [G pixel: 0.036918]\n",
      "[Epoch 26/5000] [Batch 10/14] [G pixel: 0.040924]\n",
      "[Epoch 26/5000] [Batch 11/14] [G pixel: 0.040510]\n",
      "[Epoch 26/5000] [Batch 12/14] [G pixel: 0.044649]\n",
      "[Epoch 26/5000] [Batch 13/14] [G pixel: 0.037039]\n",
      "[Epoch 27/5000] [Batch 0/14] [G pixel: 0.039395]\n",
      "[Epoch 27/5000] [Batch 1/14] [G pixel: 0.040504]\n",
      "[Epoch 27/5000] [Batch 2/14] [G pixel: 0.041160]\n",
      "[Epoch 27/5000] [Batch 3/14] [G pixel: 0.037882]\n",
      "[Epoch 27/5000] [Batch 4/14] [G pixel: 0.047181]\n",
      "[Epoch 27/5000] [Batch 5/14] [G pixel: 0.047291]\n",
      "[Epoch 27/5000] [Batch 6/14] [G pixel: 0.043249]\n",
      "[Epoch 27/5000] [Batch 7/14] [G pixel: 0.036841]\n",
      "[Epoch 27/5000] [Batch 8/14] [G pixel: 0.048806]\n",
      "[Epoch 27/5000] [Batch 9/14] [G pixel: 0.034407]\n",
      "[Epoch 27/5000] [Batch 10/14] [G pixel: 0.033966]\n",
      "[Epoch 27/5000] [Batch 11/14] [G pixel: 0.036407]\n",
      "[Epoch 27/5000] [Batch 12/14] [G pixel: 0.033707]\n",
      "[Epoch 27/5000] [Batch 13/14] [G pixel: 0.047676]\n",
      "[Epoch 28/5000] [Batch 0/14] [G pixel: 0.040560]\n",
      "[Epoch 28/5000] [Batch 1/14] [G pixel: 0.040790]\n",
      "[Epoch 28/5000] [Batch 2/14] [G pixel: 0.043270]\n",
      "[Epoch 28/5000] [Batch 3/14] [G pixel: 0.033994]\n",
      "[Epoch 28/5000] [Batch 4/14] [G pixel: 0.045438]\n",
      "[Epoch 28/5000] [Batch 5/14] [G pixel: 0.036953]\n",
      "[Epoch 28/5000] [Batch 6/14] [G pixel: 0.032982]\n",
      "[Epoch 28/5000] [Batch 7/14] [G pixel: 0.036259]\n",
      "[Epoch 28/5000] [Batch 8/14] [G pixel: 0.036413]\n",
      "[Epoch 28/5000] [Batch 9/14] [G pixel: 0.041122]\n",
      "[Epoch 28/5000] [Batch 10/14] [G pixel: 0.040933]\n",
      "[Epoch 28/5000] [Batch 11/14] [G pixel: 0.037618]\n",
      "[Epoch 28/5000] [Batch 12/14] [G pixel: 0.037606]\n",
      "[Epoch 28/5000] [Batch 13/14] [G pixel: 0.052334]\n",
      "[Epoch 29/5000] [Batch 0/14] [G pixel: 0.040886]\n",
      "[Epoch 29/5000] [Batch 1/14] [G pixel: 0.041375]\n",
      "[Epoch 29/5000] [Batch 2/14] [G pixel: 0.033810]\n",
      "[Epoch 29/5000] [Batch 3/14] [G pixel: 0.034437]\n",
      "[Epoch 29/5000] [Batch 4/14] [G pixel: 0.043508]\n",
      "[Epoch 29/5000] [Batch 5/14] [G pixel: 0.043370]\n",
      "[Epoch 29/5000] [Batch 6/14] [G pixel: 0.043034]\n",
      "[Epoch 29/5000] [Batch 7/14] [G pixel: 0.037095]\n",
      "[Epoch 29/5000] [Batch 8/14] [G pixel: 0.039083]\n",
      "[Epoch 29/5000] [Batch 9/14] [G pixel: 0.045164]\n",
      "[Epoch 29/5000] [Batch 10/14] [G pixel: 0.039851]\n",
      "[Epoch 29/5000] [Batch 11/14] [G pixel: 0.037364]\n",
      "[Epoch 29/5000] [Batch 12/14] [G pixel: 0.034419]\n",
      "[Epoch 29/5000] [Batch 13/14] [G pixel: 0.056981]\n",
      "[Epoch 30/5000] [Batch 0/14] [G pixel: 0.034405]\n",
      "[Epoch 30/5000] [Batch 1/14] [G pixel: 0.035901]\n",
      "[Epoch 30/5000] [Batch 2/14] [G pixel: 0.032604]\n",
      "[Epoch 30/5000] [Batch 3/14] [G pixel: 0.037012]\n",
      "[Epoch 30/5000] [Batch 4/14] [G pixel: 0.035937]\n",
      "[Epoch 30/5000] [Batch 5/14] [G pixel: 0.038021]\n",
      "[Epoch 30/5000] [Batch 6/14] [G pixel: 0.037627]\n",
      "[Epoch 30/5000] [Batch 7/14] [G pixel: 0.040195]\n",
      "[Epoch 30/5000] [Batch 8/14] [G pixel: 0.035771]\n",
      "[Epoch 30/5000] [Batch 9/14] [G pixel: 0.041197]\n",
      "[Epoch 30/5000] [Batch 10/14] [G pixel: 0.040197]\n",
      "[Epoch 30/5000] [Batch 11/14] [G pixel: 0.030222]\n",
      "[Epoch 30/5000] [Batch 12/14] [G pixel: 0.045206]\n",
      "[Epoch 30/5000] [Batch 13/14] [G pixel: 0.034213]\n",
      "[Epoch 31/5000] [Batch 0/14] [G pixel: 0.029565]\n",
      "[Epoch 31/5000] [Batch 1/14] [G pixel: 0.036185]\n",
      "[Epoch 31/5000] [Batch 2/14] [G pixel: 0.036276]\n",
      "[Epoch 31/5000] [Batch 3/14] [G pixel: 0.037778]\n",
      "[Epoch 31/5000] [Batch 4/14] [G pixel: 0.034238]\n",
      "[Epoch 31/5000] [Batch 5/14] [G pixel: 0.036859]\n",
      "[Epoch 31/5000] [Batch 6/14] [G pixel: 0.038802]\n",
      "[Epoch 31/5000] [Batch 7/14] [G pixel: 0.036607]\n",
      "[Epoch 31/5000] [Batch 8/14] [G pixel: 0.031460]\n",
      "[Epoch 31/5000] [Batch 9/14] [G pixel: 0.039203]\n",
      "[Epoch 31/5000] [Batch 10/14] [G pixel: 0.036179]\n",
      "[Epoch 31/5000] [Batch 11/14] [G pixel: 0.038260]\n",
      "[Epoch 31/5000] [Batch 12/14] [G pixel: 0.033085]\n",
      "[Epoch 31/5000] [Batch 13/14] [G pixel: 0.030476]\n",
      "[Epoch 32/5000] [Batch 0/14] [G pixel: 0.036057]\n",
      "[Epoch 32/5000] [Batch 1/14] [G pixel: 0.034721]\n",
      "[Epoch 32/5000] [Batch 2/14] [G pixel: 0.041378]\n",
      "[Epoch 32/5000] [Batch 3/14] [G pixel: 0.031396]\n",
      "[Epoch 32/5000] [Batch 4/14] [G pixel: 0.035264]\n",
      "[Epoch 32/5000] [Batch 5/14] [G pixel: 0.034368]\n",
      "[Epoch 32/5000] [Batch 6/14] [G pixel: 0.039810]\n",
      "[Epoch 32/5000] [Batch 7/14] [G pixel: 0.034083]\n",
      "[Epoch 32/5000] [Batch 8/14] [G pixel: 0.041607]\n",
      "[Epoch 32/5000] [Batch 9/14] [G pixel: 0.042267]\n",
      "[Epoch 32/5000] [Batch 10/14] [G pixel: 0.039840]\n",
      "[Epoch 32/5000] [Batch 11/14] [G pixel: 0.042055]\n",
      "[Epoch 32/5000] [Batch 12/14] [G pixel: 0.044453]\n",
      "[Epoch 32/5000] [Batch 13/14] [G pixel: 0.031986]\n",
      "[Epoch 33/5000] [Batch 0/14] [G pixel: 0.036622]\n",
      "[Epoch 33/5000] [Batch 1/14] [G pixel: 0.037177]\n",
      "[Epoch 33/5000] [Batch 2/14] [G pixel: 0.031941]\n",
      "[Epoch 33/5000] [Batch 3/14] [G pixel: 0.032401]\n",
      "[Epoch 33/5000] [Batch 4/14] [G pixel: 0.036063]\n",
      "[Epoch 33/5000] [Batch 5/14] [G pixel: 0.035553]\n",
      "[Epoch 33/5000] [Batch 6/14] [G pixel: 0.033196]\n",
      "[Epoch 33/5000] [Batch 7/14] [G pixel: 0.035450]\n",
      "[Epoch 33/5000] [Batch 8/14] [G pixel: 0.037650]\n",
      "[Epoch 33/5000] [Batch 9/14] [G pixel: 0.030739]\n",
      "[Epoch 33/5000] [Batch 10/14] [G pixel: 0.035212]\n",
      "[Epoch 33/5000] [Batch 11/14] [G pixel: 0.037038]\n",
      "[Epoch 33/5000] [Batch 12/14] [G pixel: 0.038554]\n",
      "[Epoch 33/5000] [Batch 13/14] [G pixel: 0.025650]\n",
      "[Epoch 34/5000] [Batch 0/14] [G pixel: 0.038796]\n",
      "[Epoch 34/5000] [Batch 1/14] [G pixel: 0.037795]\n",
      "[Epoch 34/5000] [Batch 2/14] [G pixel: 0.035046]\n",
      "[Epoch 34/5000] [Batch 3/14] [G pixel: 0.040400]\n",
      "[Epoch 34/5000] [Batch 4/14] [G pixel: 0.039128]\n",
      "[Epoch 34/5000] [Batch 5/14] [G pixel: 0.037929]\n",
      "[Epoch 34/5000] [Batch 6/14] [G pixel: 0.032077]\n",
      "[Epoch 34/5000] [Batch 7/14] [G pixel: 0.039722]\n",
      "[Epoch 34/5000] [Batch 8/14] [G pixel: 0.036815]\n",
      "[Epoch 34/5000] [Batch 9/14] [G pixel: 0.033518]\n",
      "[Epoch 34/5000] [Batch 10/14] [G pixel: 0.035114]\n",
      "[Epoch 34/5000] [Batch 11/14] [G pixel: 0.037537]\n",
      "[Epoch 34/5000] [Batch 12/14] [G pixel: 0.041022]\n",
      "[Epoch 34/5000] [Batch 13/14] [G pixel: 0.030320]\n",
      "[Epoch 35/5000] [Batch 0/14] [G pixel: 0.041701]\n",
      "[Epoch 35/5000] [Batch 1/14] [G pixel: 0.038138]\n",
      "[Epoch 35/5000] [Batch 2/14] [G pixel: 0.037666]\n",
      "[Epoch 35/5000] [Batch 3/14] [G pixel: 0.042038]\n",
      "[Epoch 35/5000] [Batch 4/14] [G pixel: 0.043857]\n",
      "[Epoch 35/5000] [Batch 5/14] [G pixel: 0.044183]\n",
      "[Epoch 35/5000] [Batch 6/14] [G pixel: 0.037782]\n",
      "[Epoch 35/5000] [Batch 7/14] [G pixel: 0.048566]\n",
      "[Epoch 35/5000] [Batch 8/14] [G pixel: 0.030321]\n",
      "[Epoch 35/5000] [Batch 9/14] [G pixel: 0.036470]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 111030<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04741e4c90c4c51a26720888d4d1329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.10MB of 0.10MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ssweere/xmm-superres/sr/notebooks/wandb/run-20210513_174913-jptn01bq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ssweere/xmm-superres/sr/notebooks/wandb/run-20210513_174913-jptn01bq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>11782</td></tr><tr><td>_timestamp</td><td>1620932735</td></tr><tr><td>_step</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr><tr><td>_step</td><td></td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">sage-flower-20</strong>: <a href=\"https://wandb.ai/samsweere/esr_gan/runs/jptn01bq\" target=\"_blank\">https://wandb.ai/samsweere/esr_gan/runs/jptn01bq</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 128, 128] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-78fbf03a599a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build, train and analyze the model with the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-f09c711bc101>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# and use them to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_GAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_pixel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#       # and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-d06450dc8902>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, feature_extractor, dataloaders, criterion_GAN, criterion_content, criterion_pixel, optimizer_G, optimizer_D, Tensor, run_path, config)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Content loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mgen_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mreal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mloss_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ddc5aec14c2e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19_54\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 128, 128] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc17f80-d8c2-4176-9248-e26ece1eec0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3028f-e16e-4a89-9309-5b4a4ac8d6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78031974-c0ee-4897-87f0-03f1d1d19482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b80065-f40d-4aed-b1c4-fe4b7643b596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
